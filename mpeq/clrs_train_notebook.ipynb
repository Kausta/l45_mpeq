{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3acdd8b",
   "metadata": {},
   "source": [
    "# Run training of CLRS algorithm\n",
    "\n",
    "Copied main function from clrs_train.py. Helper functions are located in clrs_train_funcs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b11198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import shutil\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import logging\n",
    "import clrs\n",
    "import jax\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import haiku as hk\n",
    "\n",
    "import model\n",
    "import flags\n",
    "from clrs_train_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771141dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa63cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "ch = logging.StreamHandler()\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b412628f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 02:13:06,764 - jax._src.lib.xla_bridge - INFO - Remote TPU is not linked into jax; skipping remote TPU.\n",
      "2023-03-08 02:13:06,765 - jax._src.lib.xla_bridge - INFO - Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'\n",
      "2023-03-08 02:13:06,765 - jax._src.lib.xla_bridge - INFO - Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2023-03-08 02:13:06,765 - jax._src.lib.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2023-03-08 02:13:06,766 - jax._src.lib.xla_bridge - INFO - Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2023-03-08 02:13:06,766 - jax._src.lib.xla_bridge - INFO - Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.hint_mode == 'encoded_decoded':\n",
    "    encode_hints = True\n",
    "    decode_hints = True\n",
    "elif FLAGS.hint_mode == 'decoded_only':\n",
    "    encode_hints = False\n",
    "    decode_hints = True\n",
    "elif FLAGS.hint_mode == 'none':\n",
    "    encode_hints = False\n",
    "    decode_hints = False\n",
    "else:\n",
    "    raise ValueError(\n",
    "        'Hint mode not in {encoded_decoded, decoded_only, none}.')\n",
    "\n",
    "train_lengths = [int(x) for x in FLAGS.train_lengths]\n",
    "\n",
    "rng = np.random.RandomState(FLAGS.seed)\n",
    "rng_key = jax.random.PRNGKey(rng.randint(2**32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a79e1217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 02:13:07,306 - root - INFO - Creating samplers for algo binary_search\n",
      "2023-03-08 02:13:07,312 - absl - WARNING - Ignoring kwargs {'p', 'length_needle'} when building sampler class <class 'clrs._src.samplers.SearchSampler'>\n",
      "2023-03-08 02:13:07,317 - absl - WARNING - Sampling dataset on-the-fly, unlimited samples.\n",
      "2023-03-08 02:13:07,444 - absl - WARNING - Ignoring kwargs {'p', 'length_needle'} when building sampler class <class 'clrs._src.samplers.SearchSampler'>\n",
      "2023-03-08 02:13:07,445 - absl - WARNING - Sampling dataset on-the-fly, unlimited samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 02:13:07,567 - absl - WARNING - Ignoring kwargs {'p', 'length_needle'} when building sampler class <class 'clrs._src.samplers.SearchSampler'>\n",
      "2023-03-08 02:13:07,568 - absl - WARNING - Sampling dataset on-the-fly, unlimited samples.\n",
      "2023-03-08 02:13:07,692 - absl - WARNING - Ignoring kwargs {'p', 'length_needle'} when building sampler class <class 'clrs._src.samplers.SearchSampler'>\n",
      "2023-03-08 02:13:07,693 - absl - WARNING - Sampling dataset on-the-fly, unlimited samples.\n",
      "2023-03-08 02:13:07,823 - absl - WARNING - Ignoring kwargs {'p', 'length_needle'} when building sampler class <class 'clrs._src.samplers.SearchSampler'>\n",
      "2023-03-08 02:13:07,823 - absl - WARNING - Sampling dataset on-the-fly, unlimited samples.\n",
      "2023-03-08 02:13:07,956 - absl - WARNING - Ignoring kwargs {'p', 'length_needle'} when building sampler class <class 'clrs._src.samplers.SearchSampler'>\n",
      "2023-03-08 02:13:07,957 - absl - INFO - Creating a dataset with 4096 samples.\n",
      "2023-03-08 02:13:08,175 - absl - INFO - 1000 samples created\n",
      "2023-03-08 02:13:08,302 - absl - INFO - 2000 samples created\n",
      "2023-03-08 02:13:08,434 - absl - INFO - 3000 samples created\n",
      "2023-03-08 02:13:08,563 - absl - INFO - 4000 samples created\n",
      "2023-03-08 02:13:08,642 - root - INFO - Dataset found at /tmp/CLRS30/CLRS30_v1.0.0. Skipping download.\n",
      "2023-03-08 02:13:08,643 - absl - INFO - Load dataset info from /tmp/CLRS30/CLRS30_v1.0.0/clrs_dataset/binary_search_test/1.0.0\n",
      "2023-03-08 02:13:08,645 - absl - INFO - Load dataset info from /tmp/CLRS30/CLRS30_v1.0.0/clrs_dataset/binary_search_test/1.0.0\n",
      "2023-03-08 02:13:08,646 - absl - INFO - Reusing dataset clrs_dataset (/tmp/CLRS30/CLRS30_v1.0.0/clrs_dataset/binary_search_test/1.0.0)\n",
      "2023-03-08 02:13:08,646 - absl - INFO - Constructing tf.data.Dataset clrs_dataset for split test, from /tmp/CLRS30/CLRS30_v1.0.0/clrs_dataset/binary_search_test/1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/james/.pyenv/versions/3.9.16/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 02:13:08,702 - tensorflow - WARNING - From /Users/james/.pyenv/versions/3.9.16/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2023-03-08 02:13:08.842100: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# Create samplers\n",
    "(train_samplers,\n",
    " val_samplers, val_sample_counts,\n",
    " test_samplers, test_sample_counts,\n",
    " spec_list) = create_samplers(rng, train_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3080605",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_factory = model.get_processor_factory(\n",
    "    FLAGS.processor_type,\n",
    "    use_ln=FLAGS.use_ln,\n",
    "    nb_triplet_fts=FLAGS.nb_triplet_fts,\n",
    "    nb_heads=FLAGS.nb_heads\n",
    ")\n",
    "model_params = dict(\n",
    "    processor_factory=processor_factory,\n",
    "    hidden_dim=FLAGS.hidden_size,\n",
    "    encode_hints=encode_hints,\n",
    "    decode_hints=decode_hints,\n",
    "    encoder_init=FLAGS.encoder_init,\n",
    "    use_lstm=FLAGS.use_lstm,\n",
    "    learning_rate=FLAGS.learning_rate,\n",
    "    grad_clip_max_norm=FLAGS.grad_clip_max_norm,\n",
    "    checkpoint_path=FLAGS.checkpoint_path,\n",
    "    freeze_processor=FLAGS.freeze_processor,\n",
    "    dropout_prob=FLAGS.dropout_prob,\n",
    "    hint_teacher_forcing=FLAGS.hint_teacher_forcing,\n",
    "    hint_repred_mode=FLAGS.hint_repred_mode,\n",
    "    nb_msg_passing_steps=FLAGS.nb_msg_passing_steps,\n",
    "    l1_weight=FLAGS.l1_weight\n",
    ")\n",
    "\n",
    "eval_model = model.BaselineMsgModel(\n",
    "    spec=spec_list,\n",
    "    dummy_trajectory=[next(t) for t in val_samplers],\n",
    "    **model_params\n",
    ")\n",
    "# # we will never used chunked training\n",
    "# if FLAGS.chunked_training:\n",
    "#     train_model = clrs.models.BaselineModelChunked(\n",
    "#         spec=spec_list,\n",
    "#         dummy_trajectory=[next(t) for t in train_samplers],\n",
    "#         **model_params\n",
    "#     )\n",
    "# else:\n",
    "#     train_model = eval_model\n",
    "train_model = eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6738fd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 00:34:14,765 - root - INFO - Algo binary_search step 0 current loss 26.699396, current_train_items 32.\n",
      "2023-03-08 00:34:17,156 - root - INFO - (val) algo binary_search step 0: {'return': 0.065673828125, 'score': 0.065673828125, 'examples_seen': 32, 'step': 0, 'algorithm': 'binary_search'}\n",
      "2023-03-08 00:34:17,157 - root - INFO - Checkpointing best model, best avg val score was -1.000, current avg val score is 0.066, val scores are: binary_search: 0.066\n",
      "2023-03-08 00:34:20,726 - root - INFO - Algo binary_search step 1 current loss 29.408669, current_train_items 64.\n",
      "2023-03-08 00:34:24,353 - root - INFO - Algo binary_search step 2 current loss 33.881187, current_train_items 96.\n",
      "2023-03-08 00:34:27,933 - root - INFO - Algo binary_search step 3 current loss 30.668726, current_train_items 128.\n",
      "2023-03-08 00:34:31,602 - root - INFO - Algo binary_search step 4 current loss 28.633110, current_train_items 160.\n",
      "2023-03-08 00:34:31,619 - root - INFO - Algo binary_search step 5 current loss 13.091773, current_train_items 192.\n",
      "2023-03-08 00:34:31,654 - root - INFO - Algo binary_search step 6 current loss 16.104502, current_train_items 224.\n",
      "2023-03-08 00:34:31,723 - root - INFO - Algo binary_search step 7 current loss 19.645412, current_train_items 256.\n",
      "2023-03-08 00:34:31,805 - root - INFO - Algo binary_search step 8 current loss 19.531822, current_train_items 288.\n",
      "2023-03-08 00:34:31,913 - root - INFO - Algo binary_search step 9 current loss 19.444031, current_train_items 320.\n",
      "2023-03-08 00:34:31,926 - root - INFO - Algo binary_search step 10 current loss 8.285131, current_train_items 352.\n",
      "2023-03-08 00:34:31,959 - root - INFO - Algo binary_search step 11 current loss 10.881456, current_train_items 384.\n",
      "2023-03-08 00:34:32,029 - root - INFO - Algo binary_search step 12 current loss 13.283594, current_train_items 416.\n",
      "2023-03-08 00:34:32,113 - root - INFO - Algo binary_search step 13 current loss 13.594740, current_train_items 448.\n",
      "2023-03-08 00:34:32,220 - root - INFO - Algo binary_search step 14 current loss 14.525042, current_train_items 480.\n",
      "2023-03-08 00:34:32,232 - root - INFO - Algo binary_search step 15 current loss 6.625579, current_train_items 512.\n",
      "2023-03-08 00:34:32,267 - root - INFO - Algo binary_search step 16 current loss 9.171875, current_train_items 544.\n",
      "2023-03-08 00:34:32,336 - root - INFO - Algo binary_search step 17 current loss 11.383139, current_train_items 576.\n",
      "2023-03-08 00:34:32,445 - root - INFO - Algo binary_search step 18 current loss 11.971326, current_train_items 608.\n",
      "2023-03-08 00:34:32,552 - root - INFO - Algo binary_search step 19 current loss 13.172688, current_train_items 640.\n",
      "2023-03-08 00:34:32,567 - root - INFO - Algo binary_search step 20 current loss 5.713338, current_train_items 672.\n",
      "2023-03-08 00:34:32,601 - root - INFO - Algo binary_search step 21 current loss 8.486372, current_train_items 704.\n",
      "2023-03-08 00:34:32,671 - root - INFO - Algo binary_search step 22 current loss 10.504798, current_train_items 736.\n",
      "2023-03-08 00:34:32,754 - root - INFO - Algo binary_search step 23 current loss 11.072564, current_train_items 768.\n",
      "2023-03-08 00:34:32,862 - root - INFO - Algo binary_search step 24 current loss 12.080339, current_train_items 800.\n",
      "2023-03-08 00:34:32,874 - root - INFO - Algo binary_search step 25 current loss 5.523953, current_train_items 832.\n",
      "2023-03-08 00:34:32,908 - root - INFO - Algo binary_search step 26 current loss 7.750464, current_train_items 864.\n",
      "2023-03-08 00:34:32,979 - root - INFO - Algo binary_search step 27 current loss 9.832147, current_train_items 896.\n",
      "2023-03-08 00:34:33,061 - root - INFO - Algo binary_search step 28 current loss 10.373837, current_train_items 928.\n",
      "2023-03-08 00:34:33,168 - root - INFO - Algo binary_search step 29 current loss 11.444696, current_train_items 960.\n",
      "2023-03-08 00:34:33,181 - root - INFO - Algo binary_search step 30 current loss 5.050618, current_train_items 992.\n",
      "2023-03-08 00:34:33,217 - root - INFO - Algo binary_search step 31 current loss 7.356020, current_train_items 1024.\n",
      "2023-03-08 00:34:33,286 - root - INFO - Algo binary_search step 32 current loss 9.142894, current_train_items 1056.\n",
      "2023-03-08 00:34:33,370 - root - INFO - Algo binary_search step 33 current loss 10.061143, current_train_items 1088.\n",
      "2023-03-08 00:34:33,503 - root - INFO - Algo binary_search step 34 current loss 10.686758, current_train_items 1120.\n",
      "2023-03-08 00:34:33,517 - root - INFO - Algo binary_search step 35 current loss 4.572309, current_train_items 1152.\n",
      "2023-03-08 00:34:33,550 - root - INFO - Algo binary_search step 36 current loss 7.248870, current_train_items 1184.\n",
      "2023-03-08 00:34:33,619 - root - INFO - Algo binary_search step 37 current loss 8.657798, current_train_items 1216.\n",
      "2023-03-08 00:34:33,702 - root - INFO - Algo binary_search step 38 current loss 9.515091, current_train_items 1248.\n",
      "2023-03-08 00:34:33,811 - root - INFO - Algo binary_search step 39 current loss 10.466439, current_train_items 1280.\n",
      "2023-03-08 00:34:33,825 - root - INFO - Algo binary_search step 40 current loss 4.373319, current_train_items 1312.\n",
      "2023-03-08 00:34:33,859 - root - INFO - Algo binary_search step 41 current loss 6.712300, current_train_items 1344.\n",
      "2023-03-08 00:34:33,928 - root - INFO - Algo binary_search step 42 current loss 8.700054, current_train_items 1376.\n",
      "2023-03-08 00:34:34,010 - root - INFO - Algo binary_search step 43 current loss 9.326328, current_train_items 1408.\n",
      "2023-03-08 00:34:34,118 - root - INFO - Algo binary_search step 44 current loss 10.166062, current_train_items 1440.\n",
      "2023-03-08 00:34:34,131 - root - INFO - Algo binary_search step 45 current loss 4.075942, current_train_items 1472.\n",
      "2023-03-08 00:34:34,166 - root - INFO - Algo binary_search step 46 current loss 6.500760, current_train_items 1504.\n",
      "2023-03-08 00:34:34,235 - root - INFO - Algo binary_search step 47 current loss 8.226703, current_train_items 1536.\n",
      "2023-03-08 00:34:34,316 - root - INFO - Algo binary_search step 48 current loss 9.011888, current_train_items 1568.\n",
      "2023-03-08 00:34:34,447 - root - INFO - Algo binary_search step 49 current loss 9.751478, current_train_items 1600.\n",
      "2023-03-08 00:34:34,460 - root - INFO - Algo binary_search step 50 current loss 4.258228, current_train_items 1632.\n",
      "2023-03-08 00:34:36,539 - root - INFO - (val) algo binary_search step 50: {'return': 0.193115234375, 'score': 0.193115234375, 'examples_seen': 1632, 'step': 50, 'algorithm': 'binary_search'}\n",
      "2023-03-08 00:34:36,540 - root - INFO - Checkpointing best model, best avg val score was 0.066, current avg val score is 0.193, val scores are: binary_search: 0.193\n",
      "2023-03-08 00:34:36,581 - root - INFO - Algo binary_search step 51 current loss 6.158525, current_train_items 1664.\n",
      "2023-03-08 00:34:36,651 - root - INFO - Algo binary_search step 52 current loss 7.874872, current_train_items 1696.\n",
      "2023-03-08 00:34:36,736 - root - INFO - Algo binary_search step 53 current loss 8.379261, current_train_items 1728.\n",
      "2023-03-08 00:34:36,844 - root - INFO - Algo binary_search step 54 current loss 9.354588, current_train_items 1760.\n",
      "2023-03-08 00:34:36,857 - root - INFO - Algo binary_search step 55 current loss 4.243419, current_train_items 1792.\n",
      "2023-03-08 00:34:36,892 - root - INFO - Algo binary_search step 56 current loss 5.405015, current_train_items 1824.\n",
      "2023-03-08 00:34:36,960 - root - INFO - Algo binary_search step 57 current loss 7.046929, current_train_items 1856.\n",
      "2023-03-08 00:34:37,044 - root - INFO - Algo binary_search step 58 current loss 7.748195, current_train_items 1888.\n",
      "2023-03-08 00:34:37,151 - root - INFO - Algo binary_search step 59 current loss 8.148240, current_train_items 1920.\n",
      "2023-03-08 00:34:37,165 - root - INFO - Algo binary_search step 60 current loss 3.516604, current_train_items 1952.\n",
      "2023-03-08 00:34:37,198 - root - INFO - Algo binary_search step 61 current loss 4.573036, current_train_items 1984.\n",
      "2023-03-08 00:34:37,267 - root - INFO - Algo binary_search step 62 current loss 6.150433, current_train_items 2016.\n",
      "2023-03-08 00:34:37,350 - root - INFO - Algo binary_search step 63 current loss 6.574132, current_train_items 2048.\n",
      "2023-03-08 00:34:37,491 - root - INFO - Algo binary_search step 64 current loss 7.641556, current_train_items 2080.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 00:34:37,505 - root - INFO - Algo binary_search step 65 current loss 3.746989, current_train_items 2112.\n",
      "2023-03-08 00:34:37,538 - root - INFO - Algo binary_search step 66 current loss 4.623101, current_train_items 2144.\n",
      "2023-03-08 00:34:37,609 - root - INFO - Algo binary_search step 67 current loss 5.608706, current_train_items 2176.\n",
      "2023-03-08 00:34:37,693 - root - INFO - Algo binary_search step 68 current loss 6.138997, current_train_items 2208.\n",
      "2023-03-08 00:34:37,801 - root - INFO - Algo binary_search step 69 current loss 7.137094, current_train_items 2240.\n",
      "2023-03-08 00:34:37,814 - root - INFO - Algo binary_search step 70 current loss 2.691839, current_train_items 2272.\n",
      "2023-03-08 00:34:37,848 - root - INFO - Algo binary_search step 71 current loss 4.469884, current_train_items 2304.\n",
      "2023-03-08 00:34:37,917 - root - INFO - Algo binary_search step 72 current loss 4.819474, current_train_items 2336.\n",
      "2023-03-08 00:34:37,999 - root - INFO - Algo binary_search step 73 current loss 5.284286, current_train_items 2368.\n",
      "2023-03-08 00:34:38,107 - root - INFO - Algo binary_search step 74 current loss 6.227136, current_train_items 2400.\n",
      "2023-03-08 00:34:38,120 - root - INFO - Algo binary_search step 75 current loss 1.957062, current_train_items 2432.\n",
      "2023-03-08 00:34:38,154 - root - INFO - Algo binary_search step 76 current loss 3.102086, current_train_items 2464.\n",
      "2023-03-08 00:34:38,222 - root - INFO - Algo binary_search step 77 current loss 4.457392, current_train_items 2496.\n",
      "2023-03-08 00:34:38,305 - root - INFO - Algo binary_search step 78 current loss 5.426726, current_train_items 2528.\n",
      "2023-03-08 00:34:38,435 - root - INFO - Algo binary_search step 79 current loss 6.336161, current_train_items 2560.\n",
      "2023-03-08 00:34:38,449 - root - INFO - Algo binary_search step 80 current loss 1.986102, current_train_items 2592.\n",
      "2023-03-08 00:34:38,482 - root - INFO - Algo binary_search step 81 current loss 3.492055, current_train_items 2624.\n",
      "2023-03-08 00:34:38,553 - root - INFO - Algo binary_search step 82 current loss 4.844488, current_train_items 2656.\n",
      "2023-03-08 00:34:38,637 - root - INFO - Algo binary_search step 83 current loss 4.849552, current_train_items 2688.\n",
      "2023-03-08 00:34:38,745 - root - INFO - Algo binary_search step 84 current loss 5.764262, current_train_items 2720.\n",
      "2023-03-08 00:34:38,758 - root - INFO - Algo binary_search step 85 current loss 1.567506, current_train_items 2752.\n",
      "2023-03-08 00:34:38,792 - root - INFO - Algo binary_search step 86 current loss 3.387603, current_train_items 2784.\n",
      "2023-03-08 00:34:38,862 - root - INFO - Algo binary_search step 87 current loss 5.624145, current_train_items 2816.\n",
      "2023-03-08 00:34:38,944 - root - INFO - Algo binary_search step 88 current loss 5.789497, current_train_items 2848.\n",
      "2023-03-08 00:34:39,052 - root - INFO - Algo binary_search step 89 current loss 5.475614, current_train_items 2880.\n",
      "2023-03-08 00:34:39,065 - root - INFO - Algo binary_search step 90 current loss 1.876670, current_train_items 2912.\n",
      "2023-03-08 00:34:39,100 - root - INFO - Algo binary_search step 91 current loss 2.028987, current_train_items 2944.\n",
      "2023-03-08 00:34:39,168 - root - INFO - Algo binary_search step 92 current loss 3.877215, current_train_items 2976.\n",
      "2023-03-08 00:34:39,251 - root - INFO - Algo binary_search step 93 current loss 4.958485, current_train_items 3008.\n",
      "2023-03-08 00:34:39,361 - root - INFO - Algo binary_search step 94 current loss 4.641288, current_train_items 3040.\n",
      "2023-03-08 00:34:39,393 - root - INFO - Algo binary_search step 95 current loss 1.783245, current_train_items 3072.\n",
      "2023-03-08 00:34:39,425 - root - INFO - Algo binary_search step 96 current loss 2.618838, current_train_items 3104.\n",
      "2023-03-08 00:34:39,494 - root - INFO - Algo binary_search step 97 current loss 3.944274, current_train_items 3136.\n",
      "2023-03-08 00:34:39,580 - root - INFO - Algo binary_search step 98 current loss 4.115393, current_train_items 3168.\n",
      "2023-03-08 00:34:39,689 - root - INFO - Algo binary_search step 99 current loss 4.658128, current_train_items 3200.\n",
      "2023-03-08 00:34:39,702 - root - INFO - Algo binary_search step 100 current loss 1.527453, current_train_items 3232.\n",
      "2023-03-08 00:34:41,758 - root - INFO - (val) algo binary_search step 100: {'return': 0.58544921875, 'score': 0.58544921875, 'examples_seen': 3232, 'step': 100, 'algorithm': 'binary_search'}\n",
      "2023-03-08 00:34:41,759 - root - INFO - Checkpointing best model, best avg val score was 0.193, current avg val score is 0.585, val scores are: binary_search: 0.585\n",
      "2023-03-08 00:34:41,799 - root - INFO - Algo binary_search step 101 current loss 2.856958, current_train_items 3264.\n",
      "2023-03-08 00:34:41,868 - root - INFO - Algo binary_search step 102 current loss 4.211528, current_train_items 3296.\n",
      "2023-03-08 00:34:41,949 - root - INFO - Algo binary_search step 103 current loss 4.147206, current_train_items 3328.\n",
      "2023-03-08 00:34:42,056 - root - INFO - Algo binary_search step 104 current loss 4.880200, current_train_items 3360.\n",
      "2023-03-08 00:34:42,068 - root - INFO - Algo binary_search step 105 current loss 2.040863, current_train_items 3392.\n",
      "2023-03-08 00:34:42,102 - root - INFO - Algo binary_search step 106 current loss 2.963802, current_train_items 3424.\n",
      "2023-03-08 00:34:42,170 - root - INFO - Algo binary_search step 107 current loss 4.431997, current_train_items 3456.\n",
      "2023-03-08 00:34:42,251 - root - INFO - Algo binary_search step 108 current loss 4.247244, current_train_items 3488.\n",
      "2023-03-08 00:34:42,356 - root - INFO - Algo binary_search step 109 current loss 5.864697, current_train_items 3520.\n",
      "2023-03-08 00:34:42,394 - root - INFO - Algo binary_search step 110 current loss 1.149228, current_train_items 3552.\n",
      "2023-03-08 00:34:42,426 - root - INFO - Algo binary_search step 111 current loss 2.573987, current_train_items 3584.\n",
      "2023-03-08 00:34:42,495 - root - INFO - Algo binary_search step 112 current loss 3.260364, current_train_items 3616.\n",
      "2023-03-08 00:34:42,579 - root - INFO - Algo binary_search step 113 current loss 5.010140, current_train_items 3648.\n",
      "2023-03-08 00:34:42,685 - root - INFO - Algo binary_search step 114 current loss 4.239239, current_train_items 3680.\n",
      "2023-03-08 00:34:42,699 - root - INFO - Algo binary_search step 115 current loss 1.672779, current_train_items 3712.\n",
      "2023-03-08 00:34:42,733 - root - INFO - Algo binary_search step 116 current loss 2.278889, current_train_items 3744.\n",
      "2023-03-08 00:34:42,801 - root - INFO - Algo binary_search step 117 current loss 3.108118, current_train_items 3776.\n",
      "2023-03-08 00:34:42,883 - root - INFO - Algo binary_search step 118 current loss 4.202827, current_train_items 3808.\n",
      "2023-03-08 00:34:42,989 - root - INFO - Algo binary_search step 119 current loss 4.328495, current_train_items 3840.\n",
      "2023-03-08 00:34:43,001 - root - INFO - Algo binary_search step 120 current loss 1.751626, current_train_items 3872.\n",
      "2023-03-08 00:34:43,035 - root - INFO - Algo binary_search step 121 current loss 3.126496, current_train_items 3904.\n",
      "2023-03-08 00:34:43,111 - root - INFO - Algo binary_search step 122 current loss 4.089064, current_train_items 3936.\n",
      "2023-03-08 00:34:43,196 - root - INFO - Algo binary_search step 123 current loss 3.701218, current_train_items 3968.\n",
      "2023-03-08 00:34:43,302 - root - INFO - Algo binary_search step 124 current loss 4.792232, current_train_items 4000.\n",
      "2023-03-08 00:34:43,316 - root - INFO - Algo binary_search step 125 current loss 1.464806, current_train_items 4032.\n",
      "2023-03-08 00:34:43,352 - root - INFO - Algo binary_search step 126 current loss 1.880311, current_train_items 4064.\n",
      "2023-03-08 00:34:43,497 - root - INFO - Algo binary_search step 127 current loss 3.039810, current_train_items 4096.\n",
      "2023-03-08 00:34:43,579 - root - INFO - Algo binary_search step 128 current loss 3.839820, current_train_items 4128.\n",
      "2023-03-08 00:34:43,685 - root - INFO - Algo binary_search step 129 current loss 5.124952, current_train_items 4160.\n",
      "2023-03-08 00:34:43,699 - root - INFO - Algo binary_search step 130 current loss 1.359197, current_train_items 4192.\n",
      "2023-03-08 00:34:43,730 - root - INFO - Algo binary_search step 131 current loss 2.188843, current_train_items 4224.\n",
      "2023-03-08 00:34:43,805 - root - INFO - Algo binary_search step 132 current loss 2.990061, current_train_items 4256.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 00:34:43,892 - root - INFO - Algo binary_search step 133 current loss 3.362348, current_train_items 4288.\n",
      "2023-03-08 00:34:44,000 - root - INFO - Algo binary_search step 134 current loss 3.238188, current_train_items 4320.\n",
      "2023-03-08 00:34:44,013 - root - INFO - Algo binary_search step 135 current loss 1.427442, current_train_items 4352.\n",
      "2023-03-08 00:34:44,046 - root - INFO - Algo binary_search step 136 current loss 1.516623, current_train_items 4384.\n",
      "2023-03-08 00:34:44,113 - root - INFO - Algo binary_search step 137 current loss 3.138206, current_train_items 4416.\n",
      "2023-03-08 00:34:44,197 - root - INFO - Algo binary_search step 138 current loss 3.622734, current_train_items 4448.\n",
      "2023-03-08 00:34:44,303 - root - INFO - Algo binary_search step 139 current loss 4.602278, current_train_items 4480.\n",
      "2023-03-08 00:34:44,315 - root - INFO - Algo binary_search step 140 current loss 1.105246, current_train_items 4512.\n",
      "2023-03-08 00:34:44,347 - root - INFO - Algo binary_search step 141 current loss 2.019880, current_train_items 4544.\n",
      "2023-03-08 00:34:44,447 - root - INFO - Algo binary_search step 142 current loss 3.147766, current_train_items 4576.\n",
      "2023-03-08 00:34:44,528 - root - INFO - Algo binary_search step 143 current loss 3.279491, current_train_items 4608.\n",
      "2023-03-08 00:34:44,636 - root - INFO - Algo binary_search step 144 current loss 3.957158, current_train_items 4640.\n",
      "2023-03-08 00:34:44,649 - root - INFO - Algo binary_search step 145 current loss 1.145544, current_train_items 4672.\n",
      "2023-03-08 00:34:44,683 - root - INFO - Algo binary_search step 146 current loss 2.577016, current_train_items 4704.\n",
      "2023-03-08 00:34:44,753 - root - INFO - Algo binary_search step 147 current loss 3.588899, current_train_items 4736.\n",
      "2023-03-08 00:34:44,834 - root - INFO - Algo binary_search step 148 current loss 4.446747, current_train_items 4768.\n",
      "2023-03-08 00:34:44,940 - root - INFO - Algo binary_search step 149 current loss 3.771005, current_train_items 4800.\n",
      "2023-03-08 00:34:44,952 - root - INFO - Algo binary_search step 150 current loss 1.010616, current_train_items 4832.\n",
      "2023-03-08 00:34:46,995 - root - INFO - (val) algo binary_search step 150: {'return': 0.690673828125, 'score': 0.690673828125, 'examples_seen': 4832, 'step': 150, 'algorithm': 'binary_search'}\n",
      "2023-03-08 00:34:46,996 - root - INFO - Checkpointing best model, best avg val score was 0.585, current avg val score is 0.691, val scores are: binary_search: 0.691\n",
      "2023-03-08 00:34:47,033 - root - INFO - Algo binary_search step 151 current loss 1.657837, current_train_items 4864.\n",
      "2023-03-08 00:34:47,103 - root - INFO - Algo binary_search step 152 current loss 3.463104, current_train_items 4896.\n",
      "2023-03-08 00:34:47,184 - root - INFO - Algo binary_search step 153 current loss 3.298944, current_train_items 4928.\n",
      "2023-03-08 00:34:47,290 - root - INFO - Algo binary_search step 154 current loss 4.755537, current_train_items 4960.\n",
      "2023-03-08 00:34:47,303 - root - INFO - Algo binary_search step 155 current loss 1.766609, current_train_items 4992.\n",
      "2023-03-08 00:34:47,336 - root - INFO - Algo binary_search step 156 current loss 1.956975, current_train_items 5024.\n",
      "2023-03-08 00:34:47,426 - root - INFO - Algo binary_search step 157 current loss 2.910831, current_train_items 5056.\n",
      "2023-03-08 00:34:47,507 - root - INFO - Algo binary_search step 158 current loss 3.377478, current_train_items 5088.\n",
      "2023-03-08 00:34:47,614 - root - INFO - Algo binary_search step 159 current loss 4.109368, current_train_items 5120.\n",
      "2023-03-08 00:34:47,627 - root - INFO - Algo binary_search step 160 current loss 1.054051, current_train_items 5152.\n",
      "2023-03-08 00:34:47,660 - root - INFO - Algo binary_search step 161 current loss 2.178314, current_train_items 5184.\n",
      "2023-03-08 00:34:47,728 - root - INFO - Algo binary_search step 162 current loss 4.016390, current_train_items 5216.\n",
      "2023-03-08 00:34:47,811 - root - INFO - Algo binary_search step 163 current loss 3.496745, current_train_items 5248.\n",
      "2023-03-08 00:34:47,918 - root - INFO - Algo binary_search step 164 current loss 4.504145, current_train_items 5280.\n",
      "2023-03-08 00:34:47,932 - root - INFO - Algo binary_search step 165 current loss 1.035975, current_train_items 5312.\n",
      "2023-03-08 00:34:47,965 - root - INFO - Algo binary_search step 166 current loss 1.920589, current_train_items 5344.\n",
      "2023-03-08 00:34:48,033 - root - INFO - Algo binary_search step 167 current loss 3.003425, current_train_items 5376.\n",
      "2023-03-08 00:34:48,117 - root - INFO - Algo binary_search step 168 current loss 3.612893, current_train_items 5408.\n",
      "2023-03-08 00:34:48,223 - root - INFO - Algo binary_search step 169 current loss 3.920658, current_train_items 5440.\n",
      "2023-03-08 00:34:48,236 - root - INFO - Algo binary_search step 170 current loss 1.121063, current_train_items 5472.\n",
      "2023-03-08 00:34:48,269 - root - INFO - Algo binary_search step 171 current loss 1.923858, current_train_items 5504.\n",
      "2023-03-08 00:34:48,338 - root - INFO - Algo binary_search step 172 current loss 2.709681, current_train_items 5536.\n",
      "2023-03-08 00:34:48,444 - root - INFO - Algo binary_search step 173 current loss 3.135224, current_train_items 5568.\n",
      "2023-03-08 00:34:48,551 - root - INFO - Algo binary_search step 174 current loss 3.427527, current_train_items 5600.\n",
      "2023-03-08 00:34:48,564 - root - INFO - Algo binary_search step 175 current loss 1.219334, current_train_items 5632.\n",
      "2023-03-08 00:34:48,600 - root - INFO - Algo binary_search step 176 current loss 1.799559, current_train_items 5664.\n",
      "2023-03-08 00:34:48,668 - root - INFO - Algo binary_search step 177 current loss 2.580607, current_train_items 5696.\n",
      "2023-03-08 00:34:48,752 - root - INFO - Algo binary_search step 178 current loss 3.054124, current_train_items 5728.\n",
      "2023-03-08 00:34:48,858 - root - INFO - Algo binary_search step 179 current loss 4.088357, current_train_items 5760.\n",
      "2023-03-08 00:34:48,871 - root - INFO - Algo binary_search step 180 current loss 0.657037, current_train_items 5792.\n",
      "2023-03-08 00:34:48,905 - root - INFO - Algo binary_search step 181 current loss 1.111142, current_train_items 5824.\n",
      "2023-03-08 00:34:48,974 - root - INFO - Algo binary_search step 182 current loss 2.677770, current_train_items 5856.\n",
      "2023-03-08 00:34:49,056 - root - INFO - Algo binary_search step 183 current loss 2.814154, current_train_items 5888.\n",
      "2023-03-08 00:34:49,162 - root - INFO - Algo binary_search step 184 current loss 3.544826, current_train_items 5920.\n",
      "2023-03-08 00:34:49,175 - root - INFO - Algo binary_search step 185 current loss 0.872662, current_train_items 5952.\n",
      "2023-03-08 00:34:49,207 - root - INFO - Algo binary_search step 186 current loss 1.207701, current_train_items 5984.\n",
      "2023-03-08 00:34:49,277 - root - INFO - Algo binary_search step 187 current loss 2.797060, current_train_items 6016.\n",
      "2023-03-08 00:34:49,358 - root - INFO - Algo binary_search step 188 current loss 3.159809, current_train_items 6048.\n",
      "2023-03-08 00:34:49,490 - root - INFO - Algo binary_search step 189 current loss 3.120784, current_train_items 6080.\n",
      "2023-03-08 00:34:49,504 - root - INFO - Algo binary_search step 190 current loss 1.303349, current_train_items 6112.\n",
      "2023-03-08 00:34:49,537 - root - INFO - Algo binary_search step 191 current loss 1.508982, current_train_items 6144.\n",
      "2023-03-08 00:34:49,605 - root - INFO - Algo binary_search step 192 current loss 2.407842, current_train_items 6176.\n",
      "2023-03-08 00:34:49,689 - root - INFO - Algo binary_search step 193 current loss 2.746520, current_train_items 6208.\n",
      "2023-03-08 00:34:49,796 - root - INFO - Algo binary_search step 194 current loss 3.927852, current_train_items 6240.\n",
      "2023-03-08 00:34:49,808 - root - INFO - Algo binary_search step 195 current loss 1.650279, current_train_items 6272.\n",
      "2023-03-08 00:34:49,842 - root - INFO - Algo binary_search step 196 current loss 2.565146, current_train_items 6304.\n",
      "2023-03-08 00:34:49,911 - root - INFO - Algo binary_search step 197 current loss 2.438314, current_train_items 6336.\n",
      "2023-03-08 00:34:49,993 - root - INFO - Algo binary_search step 198 current loss 3.156393, current_train_items 6368.\n",
      "2023-03-08 00:34:50,100 - root - INFO - Algo binary_search step 199 current loss 4.258679, current_train_items 6400.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 00:34:50,113 - root - INFO - Algo binary_search step 200 current loss 1.320550, current_train_items 6432.\n",
      "2023-03-08 00:34:52,173 - root - INFO - (val) algo binary_search step 200: {'return': 0.543701171875, 'score': 0.543701171875, 'examples_seen': 6432, 'step': 200, 'algorithm': 'binary_search'}\n",
      "2023-03-08 00:34:52,174 - root - INFO - Not saving new best model, best avg val score was 0.691, current avg val score is 0.544, val scores are: binary_search: 0.544\n",
      "2023-03-08 00:34:52,207 - root - INFO - Algo binary_search step 201 current loss 2.350540, current_train_items 6464.\n",
      "2023-03-08 00:34:52,276 - root - INFO - Algo binary_search step 202 current loss 3.514669, current_train_items 6496.\n",
      "2023-03-08 00:34:52,358 - root - INFO - Algo binary_search step 203 current loss 2.819115, current_train_items 6528.\n",
      "2023-03-08 00:34:52,489 - root - INFO - Algo binary_search step 204 current loss 3.543144, current_train_items 6560.\n",
      "2023-03-08 00:34:52,503 - root - INFO - Algo binary_search step 205 current loss 1.194441, current_train_items 6592.\n",
      "2023-03-08 00:34:52,537 - root - INFO - Algo binary_search step 206 current loss 1.665418, current_train_items 6624.\n",
      "2023-03-08 00:34:52,608 - root - INFO - Algo binary_search step 207 current loss 2.242723, current_train_items 6656.\n",
      "2023-03-08 00:34:52,690 - root - INFO - Algo binary_search step 208 current loss 3.107080, current_train_items 6688.\n",
      "2023-03-08 00:34:52,797 - root - INFO - Algo binary_search step 209 current loss 4.296422, current_train_items 6720.\n",
      "2023-03-08 00:34:52,810 - root - INFO - Algo binary_search step 210 current loss 0.760060, current_train_items 6752.\n",
      "2023-03-08 00:34:52,843 - root - INFO - Algo binary_search step 211 current loss 2.160259, current_train_items 6784.\n",
      "2023-03-08 00:34:52,912 - root - INFO - Algo binary_search step 212 current loss 1.786247, current_train_items 6816.\n",
      "2023-03-08 00:34:52,995 - root - INFO - Algo binary_search step 213 current loss 3.631133, current_train_items 6848.\n",
      "2023-03-08 00:34:53,102 - root - INFO - Algo binary_search step 214 current loss 3.803041, current_train_items 6880.\n",
      "2023-03-08 00:34:53,116 - root - INFO - Algo binary_search step 215 current loss 0.841754, current_train_items 6912.\n",
      "2023-03-08 00:34:53,149 - root - INFO - Algo binary_search step 216 current loss 1.144226, current_train_items 6944.\n",
      "2023-03-08 00:34:53,218 - root - INFO - Algo binary_search step 217 current loss 2.147083, current_train_items 6976.\n",
      "2023-03-08 00:34:53,300 - root - INFO - Algo binary_search step 218 current loss 2.440549, current_train_items 7008.\n",
      "2023-03-08 00:34:53,428 - root - INFO - Algo binary_search step 219 current loss 3.856927, current_train_items 7040.\n",
      "2023-03-08 00:34:53,441 - root - INFO - Algo binary_search step 220 current loss 1.114791, current_train_items 7072.\n",
      "2023-03-08 00:34:53,474 - root - INFO - Algo binary_search step 221 current loss 2.631794, current_train_items 7104.\n",
      "2023-03-08 00:34:53,543 - root - INFO - Algo binary_search step 222 current loss 1.890276, current_train_items 7136.\n",
      "2023-03-08 00:34:53,625 - root - INFO - Algo binary_search step 223 current loss 3.447057, current_train_items 7168.\n",
      "2023-03-08 00:34:53,731 - root - INFO - Algo binary_search step 224 current loss 3.345515, current_train_items 7200.\n",
      "2023-03-08 00:34:53,745 - root - INFO - Algo binary_search step 225 current loss 0.726600, current_train_items 7232.\n",
      "2023-03-08 00:34:53,778 - root - INFO - Algo binary_search step 226 current loss 1.564177, current_train_items 7264.\n",
      "2023-03-08 00:34:53,847 - root - INFO - Algo binary_search step 227 current loss 3.105916, current_train_items 7296.\n",
      "2023-03-08 00:34:53,929 - root - INFO - Algo binary_search step 228 current loss 2.584502, current_train_items 7328.\n",
      "2023-03-08 00:34:54,035 - root - INFO - Algo binary_search step 229 current loss 3.682400, current_train_items 7360.\n",
      "2023-03-08 00:34:54,048 - root - INFO - Algo binary_search step 230 current loss 0.651998, current_train_items 7392.\n",
      "2023-03-08 00:34:54,081 - root - INFO - Algo binary_search step 231 current loss 1.664360, current_train_items 7424.\n",
      "2023-03-08 00:34:54,150 - root - INFO - Algo binary_search step 232 current loss 2.319952, current_train_items 7456.\n",
      "2023-03-08 00:34:54,232 - root - INFO - Algo binary_search step 233 current loss 2.566284, current_train_items 7488.\n",
      "2023-03-08 00:34:54,337 - root - INFO - Algo binary_search step 234 current loss 3.794447, current_train_items 7520.\n",
      "2023-03-08 00:34:54,349 - root - INFO - Algo binary_search step 235 current loss 0.817280, current_train_items 7552.\n",
      "2023-03-08 00:34:54,406 - root - INFO - Algo binary_search step 236 current loss 1.550871, current_train_items 7584.\n",
      "2023-03-08 00:34:54,475 - root - INFO - Algo binary_search step 237 current loss 2.017424, current_train_items 7616.\n",
      "2023-03-08 00:34:54,558 - root - INFO - Algo binary_search step 238 current loss 2.083887, current_train_items 7648.\n",
      "2023-03-08 00:34:54,668 - root - INFO - Algo binary_search step 239 current loss 3.159379, current_train_items 7680.\n",
      "2023-03-08 00:34:54,682 - root - INFO - Algo binary_search step 240 current loss 0.909930, current_train_items 7712.\n",
      "2023-03-08 00:34:54,715 - root - INFO - Algo binary_search step 241 current loss 1.340964, current_train_items 7744.\n",
      "2023-03-08 00:34:54,784 - root - INFO - Algo binary_search step 242 current loss 2.068091, current_train_items 7776.\n",
      "2023-03-08 00:34:54,865 - root - INFO - Algo binary_search step 243 current loss 2.300102, current_train_items 7808.\n",
      "2023-03-08 00:34:54,972 - root - INFO - Algo binary_search step 244 current loss 3.618888, current_train_items 7840.\n",
      "2023-03-08 00:34:54,985 - root - INFO - Algo binary_search step 245 current loss 0.952584, current_train_items 7872.\n",
      "2023-03-08 00:34:55,019 - root - INFO - Algo binary_search step 246 current loss 0.939504, current_train_items 7904.\n",
      "2023-03-08 00:34:55,087 - root - INFO - Algo binary_search step 247 current loss 2.255000, current_train_items 7936.\n",
      "2023-03-08 00:34:55,169 - root - INFO - Algo binary_search step 248 current loss 2.989503, current_train_items 7968.\n",
      "2023-03-08 00:34:55,274 - root - INFO - Algo binary_search step 249 current loss 3.434816, current_train_items 8000.\n",
      "2023-03-08 00:34:55,286 - root - INFO - Algo binary_search step 250 current loss 1.033208, current_train_items 8032.\n",
      "2023-03-08 00:34:57,348 - root - INFO - (val) algo binary_search step 250: {'return': 0.765869140625, 'score': 0.765869140625, 'examples_seen': 8032, 'step': 250, 'algorithm': 'binary_search'}\n",
      "2023-03-08 00:34:57,348 - root - INFO - Checkpointing best model, best avg val score was 0.691, current avg val score is 0.766, val scores are: binary_search: 0.766\n",
      "2023-03-08 00:34:57,420 - root - INFO - Algo binary_search step 251 current loss 1.141741, current_train_items 8064.\n",
      "2023-03-08 00:34:57,490 - root - INFO - Algo binary_search step 252 current loss 2.064413, current_train_items 8096.\n",
      "2023-03-08 00:34:57,574 - root - INFO - Algo binary_search step 253 current loss 2.304335, current_train_items 8128.\n",
      "2023-03-08 00:34:57,680 - root - INFO - Algo binary_search step 254 current loss 2.681238, current_train_items 8160.\n",
      "2023-03-08 00:34:57,693 - root - INFO - Algo binary_search step 255 current loss 0.609960, current_train_items 8192.\n",
      "2023-03-08 00:34:57,725 - root - INFO - Algo binary_search step 256 current loss 0.981609, current_train_items 8224.\n",
      "2023-03-08 00:34:57,794 - root - INFO - Algo binary_search step 257 current loss 2.263681, current_train_items 8256.\n",
      "2023-03-08 00:34:57,876 - root - INFO - Algo binary_search step 258 current loss 2.275784, current_train_items 8288.\n",
      "2023-03-08 00:34:57,982 - root - INFO - Algo binary_search step 259 current loss 4.258018, current_train_items 8320.\n",
      "2023-03-08 00:34:57,995 - root - INFO - Algo binary_search step 260 current loss 0.589314, current_train_items 8352.\n",
      "2023-03-08 00:34:58,027 - root - INFO - Algo binary_search step 261 current loss 1.222153, current_train_items 8384.\n",
      "2023-03-08 00:34:58,097 - root - INFO - Algo binary_search step 262 current loss 1.761382, current_train_items 8416.\n",
      "2023-03-08 00:34:58,178 - root - INFO - Algo binary_search step 263 current loss 2.213659, current_train_items 8448.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 00:34:58,284 - root - INFO - Algo binary_search step 264 current loss 3.085880, current_train_items 8480.\n",
      "2023-03-08 00:34:58,297 - root - INFO - Algo binary_search step 265 current loss 0.818194, current_train_items 8512.\n",
      "2023-03-08 00:34:58,329 - root - INFO - Algo binary_search step 266 current loss 1.106221, current_train_items 8544.\n",
      "2023-03-08 00:34:58,422 - root - INFO - Algo binary_search step 267 current loss 1.694093, current_train_items 8576.\n",
      "2023-03-08 00:34:58,504 - root - INFO - Algo binary_search step 268 current loss 2.146522, current_train_items 8608.\n",
      "2023-03-08 00:34:58,611 - root - INFO - Algo binary_search step 269 current loss 2.601979, current_train_items 8640.\n",
      "2023-03-08 00:34:58,625 - root - INFO - Algo binary_search step 270 current loss 0.520964, current_train_items 8672.\n",
      "2023-03-08 00:34:58,657 - root - INFO - Algo binary_search step 271 current loss 1.433023, current_train_items 8704.\n",
      "2023-03-08 00:34:58,726 - root - INFO - Algo binary_search step 272 current loss 2.471058, current_train_items 8736.\n",
      "2023-03-08 00:34:58,807 - root - INFO - Algo binary_search step 273 current loss 2.612769, current_train_items 8768.\n",
      "2023-03-08 00:34:58,912 - root - INFO - Algo binary_search step 274 current loss 3.558712, current_train_items 8800.\n",
      "2023-03-08 00:34:58,924 - root - INFO - Algo binary_search step 275 current loss 0.317214, current_train_items 8832.\n",
      "2023-03-08 00:34:58,957 - root - INFO - Algo binary_search step 276 current loss 0.546410, current_train_items 8864.\n",
      "2023-03-08 00:34:59,025 - root - INFO - Algo binary_search step 277 current loss 2.022266, current_train_items 8896.\n",
      "2023-03-08 00:34:59,107 - root - INFO - Algo binary_search step 278 current loss 2.207521, current_train_items 8928.\n",
      "2023-03-08 00:34:59,212 - root - INFO - Algo binary_search step 279 current loss 3.647260, current_train_items 8960.\n",
      "2023-03-08 00:34:59,225 - root - INFO - Algo binary_search step 280 current loss 0.674724, current_train_items 8992.\n",
      "2023-03-08 00:34:59,258 - root - INFO - Algo binary_search step 281 current loss 0.578459, current_train_items 9024.\n",
      "2023-03-08 00:34:59,326 - root - INFO - Algo binary_search step 282 current loss 1.783676, current_train_items 9056.\n",
      "2023-03-08 00:34:59,431 - root - INFO - Algo binary_search step 283 current loss 2.040328, current_train_items 9088.\n",
      "2023-03-08 00:34:59,538 - root - INFO - Algo binary_search step 284 current loss 3.238292, current_train_items 9120.\n",
      "2023-03-08 00:34:59,552 - root - INFO - Algo binary_search step 285 current loss 0.951503, current_train_items 9152.\n",
      "2023-03-08 00:34:59,585 - root - INFO - Algo binary_search step 286 current loss 1.322087, current_train_items 9184.\n",
      "2023-03-08 00:34:59,655 - root - INFO - Algo binary_search step 287 current loss 2.149036, current_train_items 9216.\n",
      "2023-03-08 00:34:59,736 - root - INFO - Algo binary_search step 288 current loss 2.752921, current_train_items 9248.\n",
      "2023-03-08 00:34:59,841 - root - INFO - Algo binary_search step 289 current loss 2.323501, current_train_items 9280.\n",
      "2023-03-08 00:34:59,853 - root - INFO - Algo binary_search step 290 current loss 0.781131, current_train_items 9312.\n",
      "2023-03-08 00:34:59,887 - root - INFO - Algo binary_search step 291 current loss 0.953612, current_train_items 9344.\n",
      "2023-03-08 00:34:59,956 - root - INFO - Algo binary_search step 292 current loss 1.871357, current_train_items 9376.\n",
      "2023-03-08 00:35:00,039 - root - INFO - Algo binary_search step 293 current loss 2.310307, current_train_items 9408.\n",
      "2023-03-08 00:35:00,145 - root - INFO - Algo binary_search step 294 current loss 3.227034, current_train_items 9440.\n",
      "2023-03-08 00:35:00,158 - root - INFO - Algo binary_search step 295 current loss 0.346534, current_train_items 9472.\n",
      "2023-03-08 00:35:00,191 - root - INFO - Algo binary_search step 296 current loss 0.900586, current_train_items 9504.\n",
      "2023-03-08 00:35:00,259 - root - INFO - Algo binary_search step 297 current loss 1.950705, current_train_items 9536.\n",
      "2023-03-08 00:35:00,341 - root - INFO - Algo binary_search step 298 current loss 2.461701, current_train_items 9568.\n",
      "2023-03-08 00:35:00,472 - root - INFO - Algo binary_search step 299 current loss 2.883528, current_train_items 9600.\n",
      "2023-03-08 00:35:00,487 - root - INFO - Algo binary_search step 300 current loss 0.756661, current_train_items 9632.\n",
      "2023-03-08 00:35:02,541 - root - INFO - (val) algo binary_search step 300: {'return': 0.7138671875, 'score': 0.7138671875, 'examples_seen': 9632, 'step': 300, 'algorithm': 'binary_search'}\n",
      "2023-03-08 00:35:02,542 - root - INFO - Not saving new best model, best avg val score was 0.766, current avg val score is 0.714, val scores are: binary_search: 0.714\n",
      "2023-03-08 00:35:02,576 - root - INFO - Algo binary_search step 301 current loss 0.934911, current_train_items 9664.\n",
      "2023-03-08 00:35:02,645 - root - INFO - Algo binary_search step 302 current loss 2.268775, current_train_items 9696.\n",
      "2023-03-08 00:35:02,725 - root - INFO - Algo binary_search step 303 current loss 3.368687, current_train_items 9728.\n",
      "2023-03-08 00:35:02,831 - root - INFO - Algo binary_search step 304 current loss 2.658092, current_train_items 9760.\n",
      "2023-03-08 00:35:02,843 - root - INFO - Algo binary_search step 305 current loss 0.238966, current_train_items 9792.\n",
      "2023-03-08 00:35:02,877 - root - INFO - Algo binary_search step 306 current loss 2.044575, current_train_items 9824.\n",
      "2023-03-08 00:35:02,946 - root - INFO - Algo binary_search step 307 current loss 3.599066, current_train_items 9856.\n",
      "2023-03-08 00:35:03,028 - root - INFO - Algo binary_search step 308 current loss 2.757365, current_train_items 9888.\n",
      "2023-03-08 00:35:03,133 - root - INFO - Algo binary_search step 309 current loss 3.504360, current_train_items 9920.\n",
      "2023-03-08 00:35:03,146 - root - INFO - Algo binary_search step 310 current loss 0.437856, current_train_items 9952.\n",
      "2023-03-08 00:35:03,179 - root - INFO - Algo binary_search step 311 current loss 1.285735, current_train_items 9984.\n",
      "2023-03-08 00:35:03,248 - root - INFO - Algo binary_search step 312 current loss 2.179634, current_train_items 10016.\n",
      "2023-03-08 00:35:03,349 - root - INFO - Algo binary_search step 313 current loss 2.265027, current_train_items 10048.\n",
      "2023-03-08 00:35:03,456 - root - INFO - Algo binary_search step 314 current loss 2.395751, current_train_items 10080.\n",
      "2023-03-08 00:35:03,470 - root - INFO - Algo binary_search step 315 current loss 0.809399, current_train_items 10112.\n",
      "2023-03-08 00:35:03,502 - root - INFO - Algo binary_search step 316 current loss 1.225033, current_train_items 10144.\n",
      "2023-03-08 00:35:03,574 - root - INFO - Algo binary_search step 317 current loss 1.402032, current_train_items 10176.\n",
      "2023-03-08 00:35:03,656 - root - INFO - Algo binary_search step 318 current loss 1.872650, current_train_items 10208.\n",
      "2023-03-08 00:35:03,761 - root - INFO - Algo binary_search step 319 current loss 2.752181, current_train_items 10240.\n",
      "2023-03-08 00:35:03,774 - root - INFO - Algo binary_search step 320 current loss 0.461548, current_train_items 10272.\n",
      "2023-03-08 00:35:03,808 - root - INFO - Algo binary_search step 321 current loss 0.892977, current_train_items 10304.\n",
      "2023-03-08 00:35:03,876 - root - INFO - Algo binary_search step 322 current loss 1.793006, current_train_items 10336.\n",
      "2023-03-08 00:35:03,958 - root - INFO - Algo binary_search step 323 current loss 2.720691, current_train_items 10368.\n",
      "2023-03-08 00:35:04,068 - root - INFO - Algo binary_search step 324 current loss 3.359159, current_train_items 10400.\n",
      "2023-03-08 00:35:04,081 - root - INFO - Algo binary_search step 325 current loss 0.719565, current_train_items 10432.\n",
      "2023-03-08 00:35:04,113 - root - INFO - Algo binary_search step 326 current loss 0.938366, current_train_items 10464.\n",
      "2023-03-08 00:35:04,181 - root - INFO - Algo binary_search step 327 current loss 2.027555, current_train_items 10496.\n",
      "2023-03-08 00:35:04,262 - root - INFO - Algo binary_search step 328 current loss 2.335510, current_train_items 10528.\n",
      "2023-03-08 00:35:04,369 - root - INFO - Algo binary_search step 329 current loss 2.707889, current_train_items 10560.\n",
      "2023-03-08 00:35:04,382 - root - INFO - Algo binary_search step 330 current loss 1.014256, current_train_items 10592.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 00:35:04,439 - root - INFO - Algo binary_search step 331 current loss 1.295012, current_train_items 10624.\n",
      "2023-03-08 00:35:04,507 - root - INFO - Algo binary_search step 332 current loss 2.418581, current_train_items 10656.\n",
      "2023-03-08 00:35:04,590 - root - INFO - Algo binary_search step 333 current loss 3.245784, current_train_items 10688.\n",
      "2023-03-08 00:35:04,698 - root - INFO - Algo binary_search step 334 current loss 3.150851, current_train_items 10720.\n",
      "2023-03-08 00:35:04,710 - root - INFO - Algo binary_search step 335 current loss 0.589139, current_train_items 10752.\n",
      "2023-03-08 00:35:04,744 - root - INFO - Algo binary_search step 336 current loss 1.220576, current_train_items 10784.\n",
      "2023-03-08 00:35:04,813 - root - INFO - Algo binary_search step 337 current loss 2.042824, current_train_items 10816.\n",
      "2023-03-08 00:35:04,894 - root - INFO - Algo binary_search step 338 current loss 2.309875, current_train_items 10848.\n",
      "2023-03-08 00:35:05,001 - root - INFO - Algo binary_search step 339 current loss 3.025014, current_train_items 10880.\n",
      "2023-03-08 00:35:05,014 - root - INFO - Algo binary_search step 340 current loss 0.409633, current_train_items 10912.\n",
      "2023-03-08 00:35:05,046 - root - INFO - Algo binary_search step 341 current loss 1.773891, current_train_items 10944.\n",
      "2023-03-08 00:35:05,114 - root - INFO - Algo binary_search step 342 current loss 2.662872, current_train_items 10976.\n",
      "2023-03-08 00:35:05,195 - root - INFO - Algo binary_search step 343 current loss 2.103272, current_train_items 11008.\n",
      "2023-03-08 00:35:05,300 - root - INFO - Algo binary_search step 344 current loss 2.879636, current_train_items 11040.\n",
      "2023-03-08 00:35:05,313 - root - INFO - Algo binary_search step 345 current loss 1.007918, current_train_items 11072.\n",
      "2023-03-08 00:35:05,345 - root - INFO - Algo binary_search step 346 current loss 1.906831, current_train_items 11104.\n",
      "2023-03-08 00:35:05,439 - root - INFO - Algo binary_search step 347 current loss 2.327485, current_train_items 11136.\n",
      "2023-03-08 00:35:05,519 - root - INFO - Algo binary_search step 348 current loss 2.031545, current_train_items 11168.\n",
      "2023-03-08 00:35:05,626 - root - INFO - Algo binary_search step 349 current loss 3.719206, current_train_items 11200.\n",
      "2023-03-08 00:35:05,639 - root - INFO - Algo binary_search step 350 current loss 1.572873, current_train_items 11232.\n",
      "2023-03-08 00:35:08,176 - root - INFO - (val) algo binary_search step 350: {'return': 0.720458984375, 'score': 0.720458984375, 'examples_seen': 11232, 'step': 350, 'algorithm': 'binary_search'}\n",
      "2023-03-08 00:35:08,177 - root - INFO - Not saving new best model, best avg val score was 0.766, current avg val score is 0.720, val scores are: binary_search: 0.720\n",
      "2023-03-08 00:35:08,217 - root - INFO - Algo binary_search step 351 current loss 0.975336, current_train_items 11264.\n",
      "2023-03-08 00:35:08,298 - root - INFO - Algo binary_search step 352 current loss 1.948636, current_train_items 11296.\n",
      "2023-03-08 00:35:08,414 - root - INFO - Algo binary_search step 353 current loss 2.022171, current_train_items 11328.\n",
      "2023-03-08 00:35:08,615 - root - INFO - Algo binary_search step 354 current loss 3.012855, current_train_items 11360.\n",
      "2023-03-08 00:35:08,632 - root - INFO - Algo binary_search step 355 current loss 0.901644, current_train_items 11392.\n",
      "2023-03-08 00:35:08,666 - root - INFO - Algo binary_search step 356 current loss 1.561996, current_train_items 11424.\n",
      "2023-03-08 00:35:08,735 - root - INFO - Algo binary_search step 357 current loss 2.350917, current_train_items 11456.\n",
      "2023-03-08 00:35:08,818 - root - INFO - Algo binary_search step 358 current loss 2.115696, current_train_items 11488.\n",
      "2023-03-08 00:35:08,965 - root - INFO - Algo binary_search step 359 current loss 2.698202, current_train_items 11520.\n",
      "2023-03-08 00:35:08,978 - root - INFO - Algo binary_search step 360 current loss 0.411710, current_train_items 11552.\n",
      "2023-03-08 00:35:09,011 - root - INFO - Algo binary_search step 361 current loss 1.609908, current_train_items 11584.\n",
      "2023-03-08 00:35:09,080 - root - INFO - Algo binary_search step 362 current loss 1.692641, current_train_items 11616.\n",
      "2023-03-08 00:35:09,162 - root - INFO - Algo binary_search step 363 current loss 2.050999, current_train_items 11648.\n",
      "2023-03-08 00:35:09,268 - root - INFO - Algo binary_search step 364 current loss 3.264672, current_train_items 11680.\n",
      "2023-03-08 00:35:09,285 - root - INFO - Algo binary_search step 365 current loss 0.346629, current_train_items 11712.\n",
      "2023-03-08 00:35:09,359 - root - INFO - Algo binary_search step 366 current loss 0.902650, current_train_items 11744.\n",
      "2023-03-08 00:35:09,461 - root - INFO - Algo binary_search step 367 current loss 2.383917, current_train_items 11776.\n",
      "2023-03-08 00:35:09,544 - root - INFO - Algo binary_search step 368 current loss 2.747132, current_train_items 11808.\n",
      "2023-03-08 00:35:09,650 - root - INFO - Algo binary_search step 369 current loss 3.144644, current_train_items 11840.\n",
      "2023-03-08 00:35:09,665 - root - INFO - Algo binary_search step 370 current loss 0.646132, current_train_items 11872.\n",
      "2023-03-08 00:35:09,699 - root - INFO - Algo binary_search step 371 current loss 1.236053, current_train_items 11904.\n",
      "2023-03-08 00:35:09,773 - root - INFO - Algo binary_search step 372 current loss 1.179183, current_train_items 11936.\n",
      "2023-03-08 00:35:09,858 - root - INFO - Algo binary_search step 373 current loss 2.046371, current_train_items 11968.\n",
      "2023-03-08 00:35:09,988 - root - INFO - Algo binary_search step 374 current loss 2.905591, current_train_items 12000.\n",
      "2023-03-08 00:35:10,001 - root - INFO - Algo binary_search step 375 current loss 0.818052, current_train_items 12032.\n",
      "2023-03-08 00:35:10,030 - root - INFO - Algo binary_search step 376 current loss 0.935627, current_train_items 12064.\n",
      "2023-03-08 00:35:10,095 - root - INFO - Algo binary_search step 377 current loss 1.363126, current_train_items 12096.\n",
      "2023-03-08 00:35:10,172 - root - INFO - Algo binary_search step 378 current loss 1.882471, current_train_items 12128.\n",
      "2023-03-08 00:35:10,275 - root - INFO - Algo binary_search step 379 current loss 2.693334, current_train_items 12160.\n",
      "2023-03-08 00:35:10,288 - root - INFO - Algo binary_search step 380 current loss 0.292070, current_train_items 12192.\n",
      "2023-03-08 00:35:10,318 - root - INFO - Algo binary_search step 381 current loss 0.555813, current_train_items 12224.\n",
      "2023-03-08 00:35:10,384 - root - INFO - Algo binary_search step 382 current loss 1.323878, current_train_items 12256.\n",
      "2023-03-08 00:35:10,464 - root - INFO - Algo binary_search step 383 current loss 2.513428, current_train_items 12288.\n",
      "2023-03-08 00:35:10,690 - root - INFO - Algo binary_search step 384 current loss 3.883783, current_train_items 12320.\n",
      "2023-03-08 00:35:10,723 - root - INFO - Algo binary_search step 385 current loss 0.399374, current_train_items 12352.\n",
      "2023-03-08 00:35:10,767 - root - INFO - Algo binary_search step 386 current loss 0.995717, current_train_items 12384.\n",
      "2023-03-08 00:35:10,951 - root - INFO - Algo binary_search step 387 current loss 1.520273, current_train_items 12416.\n",
      "2023-03-08 00:35:11,111 - root - INFO - Algo binary_search step 388 current loss 1.889856, current_train_items 12448.\n",
      "2023-03-08 00:35:11,456 - root - INFO - Algo binary_search step 389 current loss 2.539868, current_train_items 12480.\n",
      "2023-03-08 00:35:11,472 - root - INFO - Algo binary_search step 390 current loss 0.970162, current_train_items 12512.\n",
      "2023-03-08 00:35:11,508 - root - INFO - Algo binary_search step 391 current loss 0.695782, current_train_items 12544.\n",
      "2023-03-08 00:35:11,581 - root - INFO - Algo binary_search step 392 current loss 1.444560, current_train_items 12576.\n",
      "2023-03-08 00:35:11,666 - root - INFO - Algo binary_search step 393 current loss 1.966649, current_train_items 12608.\n",
      "2023-03-08 00:35:11,777 - root - INFO - Algo binary_search step 394 current loss 2.558863, current_train_items 12640.\n",
      "2023-03-08 00:35:11,791 - root - INFO - Algo binary_search step 395 current loss 1.090445, current_train_items 12672.\n",
      "2023-03-08 00:35:11,823 - root - INFO - Algo binary_search step 396 current loss 0.386748, current_train_items 12704.\n",
      "2023-03-08 00:35:11,908 - root - INFO - Algo binary_search step 397 current loss 1.698376, current_train_items 12736.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 00:35:12,004 - root - INFO - Algo binary_search step 398 current loss 2.076700, current_train_items 12768.\n",
      "2023-03-08 00:35:12,118 - root - INFO - Algo binary_search step 399 current loss 3.815567, current_train_items 12800.\n",
      "2023-03-08 00:35:12,119 - root - INFO - Restoring best model from checkpoint...\n"
     ]
    }
   ],
   "source": [
    "# Training loop.\n",
    "best_score = -1.0\n",
    "current_train_items = [0] * len(FLAGS.algorithms)\n",
    "step = 0\n",
    "next_eval = 0\n",
    "# Make sure scores improve on first step, but not overcome best score\n",
    "# until all algos have had at least one evaluation.\n",
    "val_scores = [-99999.9] * len(FLAGS.algorithms)\n",
    "length_idx = 0\n",
    "\n",
    "while step < FLAGS.train_steps:\n",
    "    feedback_list = [next(t) for t in train_samplers]\n",
    "\n",
    "    # Initialize model.\n",
    "    if step == 0:\n",
    "        all_features = [f.features for f in feedback_list]\n",
    "        if FLAGS.chunked_training:\n",
    "            # We need to initialize the model with samples of all lengths for\n",
    "            # all algorithms. Also, we need to make sure that the order of these\n",
    "            # sample sizes is the same as the order of the actual training sizes.\n",
    "            all_length_features = [all_features] + [\n",
    "                [next(t).features for t in train_samplers]\n",
    "                for _ in range(len(train_lengths))]\n",
    "            train_model.init(all_length_features[:-1], FLAGS.seed + 1)\n",
    "        else:\n",
    "            train_model.init(all_features, FLAGS.seed + 1)\n",
    "\n",
    "    # Training step.\n",
    "    for algo_idx in range(len(train_samplers)):\n",
    "        feedback = feedback_list[algo_idx]\n",
    "        rng_key, new_rng_key = jax.random.split(rng_key)\n",
    "        if FLAGS.chunked_training:\n",
    "            # In chunked training, we must indicate which training length we are\n",
    "            # using, so the model uses the correct state.\n",
    "            length_and_algo_idx = (length_idx, algo_idx)\n",
    "        else:\n",
    "            # In non-chunked training, all training lengths can be treated equally,\n",
    "            # since there is no state to maintain between batches.\n",
    "            length_and_algo_idx = algo_idx\n",
    "        cur_loss = train_model.feedback(\n",
    "            rng_key, feedback, length_and_algo_idx)\n",
    "        rng_key = new_rng_key\n",
    "\n",
    "        if FLAGS.chunked_training:\n",
    "            examples_in_chunk = np.sum(feedback.features.is_last).item()\n",
    "        else:\n",
    "            examples_in_chunk = len(feedback.features.lengths)\n",
    "        current_train_items[algo_idx] += examples_in_chunk\n",
    "        logging.info('Algo %s step %i current loss %f, current_train_items %i.',\n",
    "                     FLAGS.algorithms[algo_idx], step,\n",
    "                     cur_loss, current_train_items[algo_idx])\n",
    "\n",
    "    # Periodically evaluate model\n",
    "    if step >= next_eval:\n",
    "        eval_model.params = train_model.params\n",
    "        for algo_idx in range(len(train_samplers)):\n",
    "            common_extras = {'examples_seen': current_train_items[algo_idx],\n",
    "                             'step': step,\n",
    "                             'algorithm': FLAGS.algorithms[algo_idx]}\n",
    "\n",
    "            # Validation info.\n",
    "            new_rng_key, rng_key = jax.random.split(rng_key)\n",
    "            val_stats = collect_and_eval(\n",
    "                val_samplers[algo_idx],\n",
    "                functools.partial(eval_model.predict,\n",
    "                                  algorithm_index=algo_idx),\n",
    "                val_sample_counts[algo_idx],\n",
    "                new_rng_key,\n",
    "                extras=common_extras)\n",
    "            logging.info('(val) algo %s step %d: %s',\n",
    "                         FLAGS.algorithms[algo_idx], step, val_stats)\n",
    "            val_scores[algo_idx] = val_stats['score']\n",
    "\n",
    "        next_eval += FLAGS.eval_every\n",
    "\n",
    "        # If best total score, update best checkpoint.\n",
    "        # Also save a best checkpoint on the first step.\n",
    "        msg = (f'best avg val score was '\n",
    "               f'{best_score/len(FLAGS.algorithms):.3f}, '\n",
    "               f'current avg val score is {np.mean(val_scores):.3f}, '\n",
    "               f'val scores are: ')\n",
    "        msg += ', '.join(\n",
    "            ['%s: %.3f' % (x, y) for (x, y) in zip(FLAGS.algorithms, val_scores)])\n",
    "        if (sum(val_scores) > best_score) or step == 0:\n",
    "            best_score = sum(val_scores)\n",
    "            logging.info('Checkpointing best model, %s', msg)\n",
    "            train_model.save_model('best.pkl')\n",
    "        else:\n",
    "            logging.info('Not saving new best model, %s', msg)\n",
    "\n",
    "    step += 1\n",
    "    length_idx = (length_idx + 1) % len(train_lengths)\n",
    "\n",
    "logging.info('Restoring best model from checkpoint...')\n",
    "eval_model.restore_model('best.pkl', only_load_processor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7414d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def restore_model(model, file_name):\n",
    "    \"\"\"Restore model from `file_name`.\"\"\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        restored_state = pickle.load(f)\n",
    "        restored_params = restored_state['params']\n",
    "        model.params = hk.data_structures.merge(restored_params)\n",
    "        model.opt_state = restored_state['opt_state']\n",
    "\n",
    "def save_model(model, file_name):\n",
    "    \"\"\"Save model (processor weights only) to `file_name`.\"\"\"\n",
    "    to_save = {'params': model.params, 'opt_state': model.opt_state}\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c11566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(eval_model, 'eval_model_asdf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb459f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_model(eval_model, 'eval_model_asdf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aed2485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'return': 0.766357421875, 'score': 0.766357421875}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_idx = 0\n",
    "common_extras = {}\n",
    "\n",
    "new_rng_key, rng_key = jax.random.split(rng_key)\n",
    "val_stats = collect_and_eval(\n",
    "    val_samplers[algo_idx],\n",
    "    functools.partial(eval_model.predict, algorithm_index=algo_idx),\n",
    "    val_sample_counts[algo_idx],\n",
    "    new_rng_key,\n",
    "    extras=common_extras)\n",
    "\n",
    "val_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2b7510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 64, 64, 128)\n",
      "6\n",
      "(5, 1, 32, 64, 64, 128)\n",
      "(7, 1, 32, 64, 64, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'return': 0.33544921875, 'score': 0.33544921875}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rng_key, rng_key = jax.random.split(rng_key)\n",
    "test_stats = collect_and_eval(\n",
    "    test_samplers[algo_idx],\n",
    "    functools.partial(eval_model.predict, algorithm_index=algo_idx),\n",
    "    test_sample_counts[algo_idx],\n",
    "    new_rng_key,\n",
    "    extras=common_extras)\n",
    "\n",
    "test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00b5f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = next(val_samplers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6eede5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_asdict',\n",
       " '_field_defaults',\n",
       " '_fields',\n",
       " '_make',\n",
       " '_replace',\n",
       " 'count',\n",
       " 'features',\n",
       " 'index',\n",
       " 'outputs']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x[:2] != '__', dir(feedback)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa70e0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_asdict',\n",
       " '_field_defaults',\n",
       " '_fields',\n",
       " '_make',\n",
       " '_replace',\n",
       " 'count',\n",
       " 'hints',\n",
       " 'index',\n",
       " 'inputs',\n",
       " 'lengths']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x[:2] != '__', dir(feedback.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87285504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Features(inputs=(DataPoint(name=\"pos\",\tlocation=node,\ttype=scalar,\tdata=Array(32, 16)), DataPoint(name=\"key\",\tlocation=node,\ttype=scalar,\tdata=Array(32, 16)), DataPoint(name=\"target\",\tlocation=graph,\ttype=scalar,\tdata=Array(32,)), DataPoint(name=\"pred\",\tlocation=node,\ttype=pointer,\tdata=Array(32, 16))), hints=(DataPoint(name=\"low\",\tlocation=node,\ttype=mask_one,\tdata=Array(5, 32, 16)), DataPoint(name=\"high\",\tlocation=node,\ttype=mask_one,\tdata=Array(5, 32, 16)), DataPoint(name=\"mid\",\tlocation=node,\ttype=mask_one,\tdata=Array(5, 32, 16))), lengths=array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
       "       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cd67ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataPoint(name=\"pos\",\tlocation=node,\ttype=scalar,\tdata=Array(32, 16)),\n",
       " DataPoint(name=\"key\",\tlocation=node,\ttype=scalar,\tdata=Array(32, 16)),\n",
       " DataPoint(name=\"target\",\tlocation=graph,\ttype=scalar,\tdata=Array(32,)),\n",
       " DataPoint(name=\"pred\",\tlocation=node,\ttype=pointer,\tdata=Array(32, 16)))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback.features.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03d5c9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14317903, 0.07972956, 0.67909305, 0.98567252, 0.93329147,\n",
       "       0.28345592, 0.43966783, 0.07153851, 0.12399225, 0.76594674,\n",
       "       0.36091068, 0.97857869, 0.28576997, 0.40300221, 0.26544856,\n",
       "       0.52667835, 0.12452213, 0.07972956, 0.93578709, 0.51574709,\n",
       "       0.3870473 , 0.58158791, 0.68471948, 0.92190682, 0.18154095,\n",
       "       0.38265265, 0.65144341, 0.42893379, 0.05218584, 0.86252671,\n",
       "       0.059794  , 0.44887807])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback.features.inputs[2].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19b9ee9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataPoint(name=\"low\",\tlocation=node,\ttype=mask_one,\tdata=Array(5, 32, 16)),\n",
       " DataPoint(name=\"high\",\tlocation=node,\ttype=mask_one,\tdata=Array(5, 32, 16)),\n",
       " DataPoint(name=\"mid\",\tlocation=node,\ttype=mask_one,\tdata=Array(5, 32, 16)))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback.features.hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b088c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(feedback.features.inputs[0].data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7396caeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),\n",
       " array([ 2,  1, 11, 15, 14,  3,  9,  0,  3, 11,  5, 15,  6,  5,  6,  6,  3,\n",
       "         1, 14,  9,  7, 12, 10, 13,  2,  5,  9, 10,  1, 14,  1,  8]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(feedback.outputs[0].data == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "189e76f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(256, 6, 64, 64, 128)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rng_key, rng_key = jax.random.split(rng_key)\n",
    "batched_msgs = get_msgs(\n",
    "    test_samplers[0],\n",
    "    functools.partial(eval_model.predict, algorithm_index=0),\n",
    "#     test_sample_counts[0],  # EXPLODING MEMORY LOL\n",
    "    32*8,\n",
    "    new_rng_key)\n",
    "\n",
    "batched_msgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1bec654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'binary_search_val_msgs.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(msgs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a3dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
